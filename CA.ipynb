{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e54967f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              STATUS  EXT  NEU  AGR  CON  OPN\n",
      "0                        likes the sound of thunder.    0    1    0    0    1\n",
      "1  is so sleepy it's not even funny that's she ca...    0    1    0    0    1\n",
      "2  is sore and wants the knot of muscles at the b...    0    1    0    0    1\n",
      "3         likes how the day sounds in this new song.    0    1    0    0    1\n",
      "4                                        is home. <3    0    1    0    0    1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATUS</th>\n",
       "      <th>EXT</th>\n",
       "      <th>NEU</th>\n",
       "      <th>AGR</th>\n",
       "      <th>CON</th>\n",
       "      <th>OPN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is so sleepy it's not even funny that's she ca...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is sore and wants the knot of muscles at the b...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>likes how the day sounds in this new song.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is home. &lt;3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>www.thejokerblogs.com</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>saw a nun zombie, and liked it. Also, *PROPNAM...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>is in Kentucky. 421 miles into her 1100 mile j...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>was about to finish a digital painting before ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>is celebrating her new haircut by listening to...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>has a crush on the Green Lantern.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>has magic on the brain.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>saw Transformers, Up, and Year One this week. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Who wants to meet up on schedule pick-up day a...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>desires the thrill of inspiration. Also, money.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               STATUS  EXT  NEU  AGR  CON  OPN\n",
       "0                         likes the sound of thunder.    0    1    0    0    1\n",
       "1   is so sleepy it's not even funny that's she ca...    0    1    0    0    1\n",
       "2   is sore and wants the knot of muscles at the b...    0    1    0    0    1\n",
       "3          likes how the day sounds in this new song.    0    1    0    0    1\n",
       "4                                         is home. <3    0    1    0    0    1\n",
       "5                               www.thejokerblogs.com    0    1    0    0    1\n",
       "6   saw a nun zombie, and liked it. Also, *PROPNAM...    0    1    0    0    1\n",
       "7   is in Kentucky. 421 miles into her 1100 mile j...    0    1    0    0    1\n",
       "8   was about to finish a digital painting before ...    0    1    0    0    1\n",
       "9   is celebrating her new haircut by listening to...    0    1    0    0    1\n",
       "10                  has a crush on the Green Lantern.    0    1    0    0    1\n",
       "11                            has magic on the brain.    0    1    0    0    1\n",
       "12  saw Transformers, Up, and Year One this week. ...    0    1    0    0    1\n",
       "13  Who wants to meet up on schedule pick-up day a...    0    1    0    0    1\n",
       "14    desires the thrill of inspiration. Also, money.    0    1    0    0    1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset (replace with your dataset file)\n",
    "data = pd.read_csv(\"mypersonality_final.csv\", encoding=\"latin1\")\n",
    "\n",
    "# Remove the first set of personality columns\n",
    "data = data.drop(columns=[\"sEXT\", \"sNEU\", \"sAGR\", \"sCON\", \"sOPN\"])\n",
    "\n",
    "# Remove the additional columns\n",
    "columns_to_remove = [\"#AUTHID\",\"NETWORKSIZE\", \"BETWEENNESS\", \"NBETWEENNESS\", \"DENSITY\", \"BROKERAGE\", \"NBROKERAGE\", \"TRANSITIVITY\"]\n",
    "data.drop(columns=columns_to_remove, inplace=True)\n",
    "\n",
    "# Replace 'n' with 0 and 'y' with 1 for the remaining columns\n",
    "columns_to_replace = [\"cEXT\", \"cNEU\", \"cAGR\", \"cCON\", \"cOPN\"]\n",
    "data[columns_to_replace] = data[columns_to_replace].replace({'n': 0, 'y': 1})\n",
    "\n",
    "# Remove the \"c\" from the columns \"sEXT,\" \"sNEU,\" \"sAGR,\" \"sCON,\" and \"sOPN\"\n",
    "data.columns = data.columns.str.replace('c', '')\n",
    "\n",
    "# Remove the \"DATE\" column\n",
    "data = data.drop(columns=[\"DATE\"])\n",
    "\n",
    "# Print the modified dataset\n",
    "print(data.head())\n",
    "\n",
    "data.head(15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e30671df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9917, 6)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5da7d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I have imported the necessary libraries and packages, including Pandas for data handling, \n",
    "#Transformers for BERT integration, TensorFlow and Keras for deep learning, and other utilities.\n",
    "import pandas as pd\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7482ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed value for reproducibility\n",
    "seed_value = 29\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac9e3e82",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "GPU device not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_37684\\151100363.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgpu_device_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'/device:GPU:0'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GPU device not found'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Found GPU at: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemError\u001b[0m: GPU device not found"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability\n",
    "#device_name = tf.test.gpu_device_name()\n",
    "#if device_name != '/device:GPU:0':\n",
    "#    raise SystemError('GPU device not found')\n",
    "#print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f47ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of personality axes and the max sequance lenght was set for the BERT tokenization. \n",
    "N_AXIS = 5  # You have 5 axes: EXT, NEU, AGR, CON, and OPN\n",
    "MAX_SEQ_LEN = 128\n",
    "BERT_NAME = 'bert-base-uncased'#base is a smaller and hence a computationally efficient version of BERT \n",
    "# compared to 'large' variants; 'uncased'means that the model is trained on text where all the words are in lowercase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd227ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = [\"EXT\", \"NEU\", \"AGR\", \"CON\", \"OPN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e67218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the text preprocessing function for eliminating square brackets, links, numbers, and emojis etc\n",
    "def text_preprocessing(text):\n",
    "    text = text.lower()#converts all characters in the input text to lowercase\n",
    "    text = re.sub('\\[.*?\\]', '', text)#remove text enclosed in square brackets\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)#removes websites \n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text.encode('ascii', 'ignore').decode('ascii')\n",
    "    if text.startswith(\"'\"):\n",
    "        text = text[1:-1]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9d8cc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac=1)  # Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "618b35ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the \"STATUS\" column and personality traits\n",
    "sentences = data[\"STATUS\"]\n",
    "personality_traits = [\"EXT\", \"NEU\", \"AGR\", \"CON\", \"OPN\"]\n",
    "labels = data[personality_traits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8e0796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ratios for train, validation, and test splits (adjust these values as needed)\n",
    "train_ratio = 0.70  # 70% for training\n",
    "val_ratio = 0.15   # 15% for validation\n",
    "test_ratio = 0.15  # 15% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "558b389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = len(data)\n",
    "train_samples = int(train_ratio * total_samples)\n",
    "val_samples = int(val_ratio * total_samples)\n",
    "test_samples = int(test_ratio * total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98b6515d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split your data into train, validation, and test sets\n",
    "train_sentences = sentences[:train_samples]\n",
    "y_train = labels[:train_samples]\n",
    "val_sentences = sentences[train_samples:train_samples + val_samples]\n",
    "y_val = labels[train_samples:train_samples + val_samples]\n",
    "test_sentences = sentences[train_samples + val_samples:train_samples + val_samples + test_samples]\n",
    "y_test = labels[train_samples + val_samples:train_samples + val_samples + test_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f098d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to prepare BERT input\n",
    "def prepare_bert_input(sentences, seq_len, bert_name):\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_name)\n",
    "    encodings = tokenizer(sentences.tolist(), truncation=True, padding='max_length',\n",
    "                          max_length=seq_len)\n",
    "    input = [np.array(encodings[\"input_ids\"]), np.array(encodings[\"token_type_ids\"]),\n",
    "             np.array(encodings[\"attention_mask\"])]\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53489834",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = prepare_bert_input(train_sentences, MAX_SEQ_LEN, BERT_NAME)\n",
    "X_val = prepare_bert_input(val_sentences, MAX_SEQ_LEN, BERT_NAME)\n",
    "X_test = prepare_bert_input(test_sentences, MAX_SEQ_LEN, BERT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1caee49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12f16e0c71e428bab6067a6fa2467d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swast\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\swast\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " token_type_ids (InputLayer)    [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'token_type_ids[0][0]',         \n",
      "                                tentions(last_hidde               'attention_mask[0][0]']         \n",
      "                                n_state=(None, 128,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 5)            3845        ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,486,085\n",
      "Trainable params: 109,486,085\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "input_ids = layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name='input_ids')\n",
    "input_type = layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name='token_type_ids')\n",
    "input_mask = layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name='attention_mask')\n",
    "inputs = [input_ids, input_type, input_mask]\n",
    "bert = TFBertModel.from_pretrained(BERT_NAME)\n",
    "bert_outputs = bert(inputs)\n",
    "last_hidden_states = bert_outputs.last_hidden_state\n",
    "avg = layers.GlobalAveragePooling1D()(last_hidden_states)\n",
    "output = layers.Dense(N_AXIS, activation=\"sigmoid\")(avg)\n",
    "model = keras.Model(inputs=inputs, outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6b1cedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " token_type_ids (InputLayer)    [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_1 (TFBertModel)  TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'token_type_ids[0][0]',         \n",
      "                                tentions(last_hidde               'attention_mask[0][0]']         \n",
      "                                n_state=(None, 128,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1 (Gl  (None, 768)         0           ['tf_bert_model_1[0][0]']        \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 5)            3845        ['global_average_pooling1d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,486,085\n",
      "Trainable params: 3,845\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT model and set its layers as non-trainable\n",
    "bert = TFBertModel.from_pretrained(BERT_NAME)\n",
    "bert.trainable = False  # Set BERT layers as non-trainable\n",
    "\n",
    "# Define the rest of the model architecture\n",
    "bert_outputs = bert(inputs)\n",
    "last_hidden_states = bert_outputs.last_hidden_state\n",
    "avg = layers.GlobalAveragePooling1D()(last_hidden_states)\n",
    "output = layers.Dense(N_AXIS, activation=\"sigmoid\")(avg)\n",
    "model = keras.Model(inputs=inputs, outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee377ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\n",
      "Epoch 1: val_auc_2 improved from -inf to 0.48536, saving model to weights.h5\n",
      "217/217 - 1365s - loss: 0.6891 - auc_2: 0.4957 - binary_accuracy: 0.5490 - val_loss: 0.6754 - val_auc_2: 0.4854 - val_binary_accuracy: 0.5847 - 1365s/epoch - 6s/step\n",
      "Epoch 2/7\n",
      "\n",
      "Epoch 2: val_auc_2 improved from 0.48536 to 0.48865, saving model to weights.h5\n",
      "217/217 - 1379s - loss: 0.6626 - auc_2: 0.4979 - binary_accuracy: 0.6007 - val_loss: 0.6672 - val_auc_2: 0.4886 - val_binary_accuracy: 0.5841 - 1379s/epoch - 6s/step\n",
      "Epoch 3/7\n"
     ]
    }
   ],
   "source": [
    "# End-to-end fine-tuning\n",
    "# Define optimizer, loss, and compile the model\n",
    "max_epochs = 7\n",
    "batch_size = 32\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "best_weights_file = \"weights.h5\"\n",
    "auc = tf.keras.metrics.AUC(multi_label=True, curve=\"ROC\")\n",
    "m_ckpt = ModelCheckpoint(best_weights_file, monitor='val_'+auc.name, mode='max', verbose=2,\n",
    "                          save_weights_only=True, save_best_only=True)\n",
    "model.compile(loss=loss, optimizer=opt, metrics=[auc, tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "# Training code remains the same\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=max_epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[m_ckpt],\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b09960",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
