{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac32aa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              STATUS  EXT  NEU  AGR  CON  OPN\n",
      "0                        likes the sound of thunder.    0    1    0    0    1\n",
      "1  is so sleepy it's not even funny that's she ca...    0    1    0    0    1\n",
      "2  is sore and wants the knot of muscles at the b...    0    1    0    0    1\n",
      "3         likes how the day sounds in this new song.    0    1    0    0    1\n",
      "4                                        is home. <3    0    1    0    0    1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATUS</th>\n",
       "      <th>EXT</th>\n",
       "      <th>NEU</th>\n",
       "      <th>AGR</th>\n",
       "      <th>CON</th>\n",
       "      <th>OPN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is so sleepy it's not even funny that's she ca...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is sore and wants the knot of muscles at the b...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>likes how the day sounds in this new song.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is home. &lt;3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>www.thejokerblogs.com</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>saw a nun zombie, and liked it. Also, *PROPNAM...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>is in Kentucky. 421 miles into her 1100 mile j...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>was about to finish a digital painting before ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>is celebrating her new haircut by listening to...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>has a crush on the Green Lantern.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>has magic on the brain.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>saw Transformers, Up, and Year One this week. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Who wants to meet up on schedule pick-up day a...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>desires the thrill of inspiration. Also, money.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               STATUS  EXT  NEU  AGR  CON  OPN\n",
       "0                         likes the sound of thunder.    0    1    0    0    1\n",
       "1   is so sleepy it's not even funny that's she ca...    0    1    0    0    1\n",
       "2   is sore and wants the knot of muscles at the b...    0    1    0    0    1\n",
       "3          likes how the day sounds in this new song.    0    1    0    0    1\n",
       "4                                         is home. <3    0    1    0    0    1\n",
       "5                               www.thejokerblogs.com    0    1    0    0    1\n",
       "6   saw a nun zombie, and liked it. Also, *PROPNAM...    0    1    0    0    1\n",
       "7   is in Kentucky. 421 miles into her 1100 mile j...    0    1    0    0    1\n",
       "8   was about to finish a digital painting before ...    0    1    0    0    1\n",
       "9   is celebrating her new haircut by listening to...    0    1    0    0    1\n",
       "10                  has a crush on the Green Lantern.    0    1    0    0    1\n",
       "11                            has magic on the brain.    0    1    0    0    1\n",
       "12  saw Transformers, Up, and Year One this week. ...    0    1    0    0    1\n",
       "13  Who wants to meet up on schedule pick-up day a...    0    1    0    0    1\n",
       "14    desires the thrill of inspiration. Also, money.    0    1    0    0    1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset (replace with your dataset file)\n",
    "data = pd.read_csv(\"mypersonality_final.csv\", encoding=\"latin1\")\n",
    "\n",
    "# Remove the first set of personality columns\n",
    "data = data.drop(columns=[\"sEXT\", \"sNEU\", \"sAGR\", \"sCON\", \"sOPN\"])\n",
    "\n",
    "# Remove the additional columns\n",
    "columns_to_remove = [\"#AUTHID\",\"NETWORKSIZE\", \"BETWEENNESS\", \"NBETWEENNESS\", \"DENSITY\", \"BROKERAGE\", \"NBROKERAGE\", \"TRANSITIVITY\"]\n",
    "data.drop(columns=columns_to_remove, inplace=True)\n",
    "\n",
    "# Replace 'n' with 0 and 'y' with 1 for the remaining columns\n",
    "columns_to_replace = [\"cEXT\", \"cNEU\", \"cAGR\", \"cCON\", \"cOPN\"]\n",
    "data[columns_to_replace] = data[columns_to_replace].replace({'n': 0, 'y': 1})\n",
    "\n",
    "# Remove the \"c\" from the columns \"cEXT\", \"cNEU\", \"cAGR\", \"cCON\", \"cOPN\"\n",
    "data.columns = data.columns.str.replace('c', '')\n",
    "\n",
    "# Remove the \"DATE\" column\n",
    "data = data.drop(columns=[\"DATE\"])\n",
    "\n",
    "# Print the modified dataset\n",
    "print(data.head())\n",
    "\n",
    "data.head(15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c349c81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9917, 6)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2f0adc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I have imported the necessary libraries and packages, including Pandas for data handling, \n",
    "#Transformers for BERT integration, TensorFlow and Keras for deep learning, and other utilities.\n",
    "import pandas as pd\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d755b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed value for reproducibility\n",
    "seed_value = 29\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e122a02",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "GPU device not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_37684\\151100363.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgpu_device_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'/device:GPU:0'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GPU device not found'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Found GPU at: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemError\u001b[0m: GPU device not found"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability\n",
    "#device_name = tf.test.gpu_device_name()\n",
    "#if device_name != '/device:GPU:0':\n",
    "#    raise SystemError('GPU device not found')\n",
    "#print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a986d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of personality axes and the max sequance lenght was set for the BERT tokenization. \n",
    "N_AXIS = 5  # You have 5 axes: EXT, NEU, AGR, CON, and OPN\n",
    "MAX_SEQ_LEN = 128\n",
    "BERT_NAME = 'bert-base-uncased'#base is a smaller and hence a computationally efficient version of BERT \n",
    "# compared to 'large' variants; 'uncased'means that the model is trained on text where all the words are in lowercase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a85a012",
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = [\"EXT\", \"NEU\", \"AGR\", \"CON\", \"OPN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2857ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the text preprocessing function for eliminating square brackets, links, numbers, and emojis etc\n",
    "def text_preprocessing(text):\n",
    "    text = text.lower()#converts all characters in the input text to lowercase\n",
    "    text = re.sub('\\[.*?\\]', '', text)#remove text enclosed in square brackets\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)#removes websites \n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text.encode('ascii', 'ignore').decode('ascii')\n",
    "    if text.startswith(\"'\"):\n",
    "        text = text[1:-1]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84c19dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac=1)  # Shuffle the data;frac=1 means that you want to sample (shuffle) the entire DataFrame, \n",
    "#which is equivalent to shuffling all the row\n",
    "#By shuffling the data, you randomize the order in which the samples are presented during training, which can help \n",
    "#the model generalize better and avoid learning patterns based on the input order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2f0984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the \"STATUS\" column and personality traits\n",
    "sentences = data[\"STATUS\"]\n",
    "personality_traits = [\"EXT\", \"NEU\", \"AGR\", \"CON\", \"OPN\"]\n",
    "labels = data[personality_traits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b06a536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ratios for train, validation, and test splits (adjust these values as needed)\n",
    "train_ratio = 0.70  # 70% for training\n",
    "val_ratio = 0.15   # 15% for validation\n",
    "test_ratio = 0.15  # 15% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66505e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of samples that will be allocated for training, validation, and testing when \n",
    "# splitting your dataset into these three subsets\n",
    "total_samples = len(data)\n",
    "train_samples = int(train_ratio * total_samples)\n",
    "val_samples = int(val_ratio * total_samples)\n",
    "test_samples = int(test_ratio * total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b772029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split your data into train, validation, and test sets\n",
    "train_sentences = sentences[:train_samples]\n",
    "y_train = labels[:train_samples]\n",
    "val_sentences = sentences[train_samples:train_samples + val_samples]\n",
    "y_val = labels[train_samples:train_samples + val_samples]\n",
    "test_sentences = sentences[train_samples + val_samples:train_samples + val_samples + test_samples]\n",
    "y_test = labels[train_samples + val_samples:train_samples + val_samples + test_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed1edda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to prepare BERT input\n",
    "def prepare_bert_input(sentences, seq_len, bert_name):\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_name)#The tokenizer is responsible for converting text data into a format that BERT can understand.\n",
    "    encodings = tokenizer(sentences.tolist(), truncation=True, padding='max_length',\n",
    "                          max_length=seq_len)\n",
    "    input = [np.array(encodings[\"input_ids\"]), np.array(encodings[\"token_type_ids\"]),\n",
    "             np.array(encodings[\"attention_mask\"])]\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6e54d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the input data for your BERT-based neural network model\n",
    "X_train = prepare_bert_input(train_sentences, MAX_SEQ_LEN, BERT_NAME)\n",
    "X_val = prepare_bert_input(val_sentences, MAX_SEQ_LEN, BERT_NAME)\n",
    "X_test = prepare_bert_input(test_sentences, MAX_SEQ_LEN, BERT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "998c9db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12f16e0c71e428bab6067a6fa2467d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swast\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\swast\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " token_type_ids (InputLayer)    [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'token_type_ids[0][0]',         \n",
      "                                tentions(last_hidde               'attention_mask[0][0]']         \n",
      "                                n_state=(None, 128,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 5)            3845        ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,486,085\n",
      "Trainable params: 109,486,085\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "input_ids = layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name='input_ids')\n",
    "input_type = layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name='token_type_ids')\n",
    "input_mask = layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name='attention_mask')\n",
    "inputs = [input_ids, input_type, input_mask]\n",
    "bert = TFBertModel.from_pretrained(BERT_NAME)\n",
    "bert_outputs = bert(inputs)\n",
    "last_hidden_states = bert_outputs.last_hidden_state\n",
    "avg = layers.GlobalAveragePooling1D()(last_hidden_states)\n",
    "output = layers.Dense(N_AXIS, activation=\"sigmoid\")(avg)\n",
    "model = keras.Model(inputs=inputs, outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91f38724",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " token_type_ids (InputLayer)    [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_1 (TFBertModel)  TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'token_type_ids[0][0]',         \n",
      "                                tentions(last_hidde               'attention_mask[0][0]']         \n",
      "                                n_state=(None, 128,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1 (Gl  (None, 768)         0           ['tf_bert_model_1[0][0]']        \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 5)            3845        ['global_average_pooling1d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,486,085\n",
      "Trainable params: 3,845\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT model and set its layers as non-trainable\n",
    "bert = TFBertModel.from_pretrained(BERT_NAME)\n",
    "bert.trainable = False  # Set BERT layers as non-trainable\n",
    "\n",
    "# Define the rest of the model architecture\n",
    "bert_outputs = bert(inputs)\n",
    "last_hidden_states = bert_outputs.last_hidden_state\n",
    "avg = layers.GlobalAveragePooling1D()(last_hidden_states)\n",
    "output = layers.Dense(N_AXIS, activation=\"sigmoid\")(avg)\n",
    "model = keras.Model(inputs=inputs, outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abd1c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layers: provides a list of all the layers in the model, including input layers(3nos)\n",
    "#BERT layers:responsible for processing the input text data and extracting contextual embeddings. It has a large number of parameters (109,482,240), \n",
    "#which are the weights learned during pre-training\n",
    "#Glabal average output layer:The output shape is (None, 768), which means it reduces the sequence length dimension to 768 while retaining all samples in the batch\n",
    "#Dense output layer: It takes the 768-dimensional output from the global average pooling layer and produces a 5-dimensional output\n",
    "#non trainable belong to the pre-trained BERT model, and their values are fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79a8ccd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\n",
      "Epoch 1: val_auc_2 improved from -inf to 0.48536, saving model to weights.h5\n",
      "217/217 - 1365s - loss: 0.6891 - auc_2: 0.4957 - binary_accuracy: 0.5490 - val_loss: 0.6754 - val_auc_2: 0.4854 - val_binary_accuracy: 0.5847 - 1365s/epoch - 6s/step\n",
      "Epoch 2/7\n",
      "\n",
      "Epoch 2: val_auc_2 improved from 0.48536 to 0.48865, saving model to weights.h5\n",
      "217/217 - 1379s - loss: 0.6626 - auc_2: 0.4979 - binary_accuracy: 0.6007 - val_loss: 0.6672 - val_auc_2: 0.4886 - val_binary_accuracy: 0.5841 - 1379s/epoch - 6s/step\n",
      "Epoch 3/7\n",
      "\n",
      "Epoch 3: val_auc_2 improved from 0.48865 to 0.48908, saving model to weights.h5\n",
      "217/217 - 1395s - loss: 0.6609 - auc_2: 0.4980 - binary_accuracy: 0.6011 - val_loss: 0.6673 - val_auc_2: 0.4891 - val_binary_accuracy: 0.5861 - 1395s/epoch - 6s/step\n",
      "Epoch 4/7\n",
      "\n",
      "Epoch 4: val_auc_2 improved from 0.48908 to 0.49004, saving model to weights.h5\n",
      "217/217 - 1319s - loss: 0.6606 - auc_2: 0.4998 - binary_accuracy: 0.6014 - val_loss: 0.6663 - val_auc_2: 0.4900 - val_binary_accuracy: 0.5861 - 1319s/epoch - 6s/step\n",
      "Epoch 5/7\n",
      "\n",
      "Epoch 5: val_auc_2 did not improve from 0.49004\n",
      "217/217 - 1347s - loss: 0.6602 - auc_2: 0.5043 - binary_accuracy: 0.6019 - val_loss: 0.6653 - val_auc_2: 0.4894 - val_binary_accuracy: 0.5853 - 1347s/epoch - 6s/step\n",
      "Epoch 6/7\n",
      "\n",
      "Epoch 6: val_auc_2 did not improve from 0.49004\n",
      "217/217 - 1278s - loss: 0.6603 - auc_2: 0.5014 - binary_accuracy: 0.6013 - val_loss: 0.6649 - val_auc_2: 0.4897 - val_binary_accuracy: 0.5879 - 1278s/epoch - 6s/step\n",
      "Epoch 7/7\n",
      "\n",
      "Epoch 7: val_auc_2 improved from 0.49004 to 0.49033, saving model to weights.h5\n",
      "217/217 - 1204s - loss: 0.6600 - auc_2: 0.5028 - binary_accuracy: 0.6013 - val_loss: 0.6644 - val_auc_2: 0.4903 - val_binary_accuracy: 0.5863 - 1204s/epoch - 6s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20c29b65a30>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# End-to-end fine-tuning\n",
    "# Define optimizer, loss, and compile the model\n",
    "max_epochs = 7\n",
    "batch_size = 32 #Smaller batch sizes, like 32, can be more computationally efficient because they require less memory (16,32, 64, etc are other batch)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=3e-5)#Adam optimizer is commonly used for training deep learning models\n",
    "loss = tf.keras.losses.BinaryCrossentropy()#loss function used to compute the model's error during training\n",
    "best_weights_file = \"weights.h5\"\n",
    "auc = tf.keras.metrics.AUC(multi_label=True, curve=\"ROC\")\n",
    "m_ckpt = ModelCheckpoint(best_weights_file, monitor='val_'+auc.name, mode='max', verbose=2,\n",
    "                          save_weights_only=True, save_best_only=True)\n",
    "model.compile(loss=loss, optimizer=opt, metrics=[auc, tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "# Training code remains the same\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=max_epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[m_ckpt],\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "935e27be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 189s 4s/step\n",
      "47/47 [==============================] - 209s 4s/step - loss: 0.6594 - auc_4: 0.4984 - binary_accuracy: 0.5978\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.659368634223938, 0.49841880798339844, 0.5978479981422424]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "model.load_weights(best_weights_file)\n",
    "model.compile(loss=loss, optimizer=opt, metrics=[tf.keras.metrics.AUC(multi_label=True, curve=\"ROC\"),\n",
    "                                                  tf.keras.metrics.BinaryAccuracy()])\n",
    "predictions = model.predict(X_test)\n",
    "model.evaluate(X_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59388c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 204ms/step\n",
      "Predicted Personality Traits: ['AGR', 'OPN']\n"
     ]
    }
   ],
   "source": [
    "# As a final step, you can use the model to predict personality traits for new sentences\n",
    "# Replace the example sentence with your own input if needed\n",
    "s1 = \"The food was not only very tasty and filling, but the portions were impressively large. Everything about our dining experience was exceptional, and the pictures speak for themselves. We have every intention of returning, especially given the reasonable price of â‚¬43 for three main courses, two beers, and a delightful dessert on the house.\"\n",
    "\n",
    "sentences = np.asarray([s1])\n",
    "enc_sentences = prepare_bert_input(sentences, MAX_SEQ_LEN, BERT_NAME)\n",
    "predictions = model.predict(enc_sentences)\n",
    "\n",
    "# Decode the predictions (0 or 1) into personality traits\n",
    "pred_traits = [axes[i] for i in range(len(axes)) if predictions[0][i] >= 0.5]\n",
    "print(\"Predicted Personality Traits:\", pred_traits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6580f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
